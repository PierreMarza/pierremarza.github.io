<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Pierre  Marza


  | Multi-Object Navigation with dynamically learned neural implicit representations

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />  -->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/projects/dynamic_implicit_representations/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Multi-Object Navigation with dynamically learned neural implicit representations",
      "description": "ICCV 2023",
      "published": "October 1, 2023",
      "authors": [
        
        {
          "author": "Pierre Marza",
          "authorURL": "https://pierremarza.github.io/",
          "affiliations": [
            {
              "name": "INSA Lyon",
              "url": ""
            }
          ]
        },
        
        {
          "author": "LaÃ«titia Matignon",
          "authorURL": "https://perso.liris.cnrs.fr/laetitia.matignon/",
          "affiliations": [
            {
              "name": "UCBL",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Olivier Simonin",
          "authorURL": "http://perso.citi-lab.fr/osimonin/",
          "affiliations": [
            {
              "name": "INSA Lyon",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Christian Wolf",
          "authorURL": "https://chriswolfvision.github.io/www/",
          "affiliations": [
            {
              "name": "Naver Labs Europe",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Pierre</span>   Marza
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/challenges/">
                Challenges
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Research
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Multi-Object Navigation with dynamically learned neural implicit representations</h1>
        <p>ICCV 2023</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p><a href="https://arxiv.org/abs/2210.05129" class="btn"><strong>Paper</strong></a>
<a href="https://github.com/PierreMarza/dynamic_implicit_representations" class="btn"><strong>Code</strong></a>
<a href="/assets/img/dynamic_implicit_representations/iccv2023_poster.pdf" class="btn"><strong>ICCV 2023 poster</strong></a>
<a href="https://youtu.be/r_F9M80GPUI" class="btn"><strong>ICCV 2023 video</strong></a></p>

<h2 id="abstract">Abstract</h2>
<p>Understanding and mapping a new environment are core abilities of any autonomously navigating agent. While classical robotics usually estimates maps in a stand-alone manner with SLAM variants, which maintain a topological or metric representation, end-to-end learning of navigation keeps some form of memory in a neural network. Networks are typically imbued with inductive biases, which can range from vectorial representations to birds-eye metric tensors or topological structures. In this work, we propose to structure neural networks with two neural implicit representations, which are learned dynamically during each episode and map the content of the scene: (i) the Semantic Finder predicts the position of a previously seen queried object; (ii) the Occupancy and Exploration Implicit Representation encapsulates information about explored area and obstacles, and is queried with a novel global read mechanism which directly maps from function space to a usable embedding space. Both representations are leveraged by an agent trained with Reinforcement Learning (RL) and learned online during each episode. We evaluate the agent on Multi-Object Navigation and show the high impact of using neural implicit representations as a memory source.</p>

<iframe width="100%" height="395" src="https://www.youtube.com/embed/r_F9M80GPUI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
</iframe>

<h2 id="multi-on">Multi-ON</h2>
<p>We target the Multi-ON task, where an agent is required to reach a sequence of target objects, more precisely coloured cylinders, in a certain order, and which was used for a
recent challenge organized in the context of the CVPR 2021 Embodied AI Workshop. Compared to much easier tasks like PointGoal or (Single) Object Navigation, Multi-ON requires
more difficult reasoning capacities, in particular mapping the position of an object once it has been seen. The following capacities are necessary to ensure optimal performance: (i) mapping the object, i.e. storing it in a suitable latent memory representation; (ii) retrieving this location on request and using it for navigation and planning, including deciding when to retrieve this information, i.e. solving a correspondence problem between sub-goals and memory representation. The agent deals with sequences of objects that are randomly placed in the environment. At each time step, it only knows the class of the next target, which is updated when reached. The episode lasts until either the agent has found all objects in the correct order or the time limit is reached.</p>

<h2 id="baselines">Baselines</h2>
<p>Multi-ON baselines (<em>NoMap</em>, <em>ProjNeuralMap</em>, <em>OracleMap</em>, <em>OracleEgoMap</em>) are presented <a href="https://pierremarza.github.io/projects/teaching_agents_how_to_map/"><strong>here</strong></a></p>

<h2 id="dynamic-implicit-representations">Dynamic Implicit Representations</h2>
<h3 id="the-semantic-finder-f_s">The Semantic Finder \(f_s\)</h3>
<p>The aim of this model is to localize an object of interest within the scene. From a query vector given as input, the Semantic Finder predicts the position of the object, which is particularly useful for an agent interacting with an environment in the context of a goal conditioned task. It is implemented as a 3-layer MLP with ReLu activations in the intermediate layers and a sigmoid activation for the output. Hidden layers have 512 neurons. The query vector q corresponds to the 1-in-K encoding of the target object class, which during navigation is directly determined by the object
goal gt provided by the task</p>

<h3 id="the-occupancy-and-exploration-implicit-representation-f_o">The Occupancy and Exploration Implicit Representation \(f_o\)</h3>
<p>Unlike \(f_s\), the occupancy representation \(f_o\) is closer to classical implicit representations in robotics, which map spatial coordinates to variables encoding information on navigable area like occupancy or signed distances. Different to previous work, our representation also includes exploration information, which changes over time during the episode. Once explored, a position changes its class, which makes our neural field dynamic. Another difference with
\(f_s\) is that the latter deals with 3D coordinates while \(f_o\) is a topdown 2D representation. Inspired by previous work, the model uses Fourier features extracted from the 2D coordinates x previously normalized \(\in [0, 1]\). The network fo is a 3-layer MLP with ReLu intermediate activations and a softmax function at the output layer. Hidden layers have 512 neurons.</p>

<h3 id="the-global-reader-r">The Global Reader \(r\)</h3>
<p>The Occupancy and Exploration Representation can in principle be queried directly for a single position, but reading out information over a large area directly this way would require multiple reads. We propose to compress this procedure by providing a trainable global read operation \(r(.; \theta_g)\), which predicts an embedding e containing a global context about what has already been explored, and positions of navigable space. The prediction is done directly from the trainable parameters of the implicit representation, as \(e = r(\theta_o; \theta_r)\). Here \(\theta_o\) is input to \(r\), whereas \(\theta_r\) are its parameters.</p>

<p>The Global Reader is implemented as a transformer model with self-attention. It takes as input a sequence of tokens \((w_1, ..., w_N )\), where \(w_i \in R^a\)
is a learned linear embedding of the incoming weights of one neuron within the implicit representation \(f_o\), and \(N\) is the number of neurons of \(f_o\). Each token is summed with a positional encoding in the form of Fourier features. An additional âCLSâ token with learned embedding is concatenated to the input sequence. The reader is composed of 4 self-attention layers, with 8 attention heads. The output representation of the âCLSâ token is used as the global embedding of the
implicit representation.</p>

<h3 id="the-rl-agent">The RL agent</h3>
<p>The agent is trained with RL, more precisely Proximal Policy Optimization (PPO) for 70M steps. The inner training loops of the implicit representations are supervised (red arrows in the figure below) and occur at each time step in the forward pass, whereas the RL-based outer training loop of the agent occur after N acting steps (black arrows in the figure below).</p>

<p><img src="/assets/img/dynamic_implicit_representations/policy.png" width="100%" /></p>

<p>Given representations \(f_s\) and \(f_o\), a single forward pass of the agent at time step \(t\) and for a goal \(g_t\) involves reading the representations and providing the input to the policy. The current RGB-D observation \(o_t\) is also encoded by the convolutional network \(c\) (different from the projection module \(p\) used to generate samples for training the Semantic Finder). Previous action \(a_{t-1}\) and current goal \(g_t\) are passed through embedding layers, named \(L(.)\) in the following equations. These different outputs are fed to the policy,</p>

<p>\(\mathbf{x}_t =  f_s(g_t;\theta_{s,t})\),</p>

<p>\(\mathbf{e}_t = r(\theta_{o,t};\theta_r)\),</p>

<p>\(\mathbf{c}_t = c(o_t; \theta_{c})\),</p>

<p>\(\mathbf{h}_t = GRU(\mathbf{h}_{t-1}, \mathbf{x}_t, u_t, \mathbf{e}_t, L(a_{t-1}), L(g_t), \mathbf{c}_t; \theta_{G})\),</p>

<p>\(a_t = \pi(\mathbf{h}_t;\theta_{\pi})\),</p>

<p>where we added indices \(\cdot_t\) to relevant variables to indicate time. Please note that the trainable parameters \(\theta_{s,t}\) and \(\theta_{o,t}\) of the two implicit representations are time dependent, as they depend on the observed scene and are updated dynamically, whereas the parameters of the policy \(\pi\) and the global reader \(r\) are not. Here, GRU corresponds to the update equations of a GRU network, where we omitted gates for ease of notation. The notation \(a_t=\pi(.)\) predicting action \(a_t\) is also a simplification, as we train the agent with PPO.</p>

<h2 id="results">Results</h2>
<p>To keep compute requirements limited and decrease sample complexity, in some experiments (see paper for more details) we do not train the full agent from scratch, in particular since the early stages of training are spent on learning basic interactions. We decompose training into three phases: 0â30M steps (no implicit representations, i.e. all entries to the agent related to \(f_s\) and \(f_o\) are set to 0); 30Mâ50M steps (training includes the Semantic Finder \(f_s\)) and finally 50Mâ70M steps (full model). This 3-steps approach is denoted as <em>curriculum</em>.</p>

<h3 id="comparison-with-baselines--with-curriculum">Comparison with baselines â with <em>curriculum</em></h3>
<p>As can be seen below, our method outperforms the different competing representations when trained with the <em>curriculum</em> scheme. We make sure that all three methods are completely comparable (by pre-training all encoders in the same way, please see paper for more details).
<img src="/assets/img/dynamic_implicit_representations/results_comp_baselines.png" width="100%" /></p>

<h3 id="comparison-with-sota--without-curriculum">Comparison with SOTA â without <em>curriculum</em></h3>
<p>Results below show that when training with both implicit representations available to the agent at the beginning (no <em>curriculum</em>) and even without any pre-training of encoders, our method outperforms the previous <a href="https://pierremarza.github.io/projects/teaching_agents_how_to_map/">state-of-the-art method</a>.
<img src="/assets/img/dynamic_implicit_representations/results_comp_sota.png" width="100%" /></p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Pierre  Marza.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib">
  </d-bibliography>

</html>
