<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Pierre  Marza


  | Task-conditioned adaptation of visual features in multi-task policy learning

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />  -->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/projects/task_conditioned_adaptation/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Task-conditioned adaptation of visual features in multi-task policy learning",
      "description": "CVPR 2024",
      "published": "June 19, 2024",
      "authors": [
        
        {
          "author": "Pierre Marza",
          "authorURL": "https://pierremarza.github.io/",
          "affiliations": [
            {
              "name": "INSA Lyon",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Laëtitia Matignon",
          "authorURL": "https://perso.liris.cnrs.fr/laetitia.matignon/",
          "affiliations": [
            {
              "name": "UCBL",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Olivier Simonin",
          "authorURL": "http://perso.citi-lab.fr/osimonin/",
          "affiliations": [
            {
              "name": "INSA Lyon",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Christian Wolf",
          "authorURL": "https://chriswolfvision.github.io/www/",
          "affiliations": [
            {
              "name": "Naver Labs Europe",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Pierre</span>   Marza
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/challenges/">
                Challenges
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Research
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Task-conditioned adaptation of visual features in multi-task policy learning</h1>
        <p>CVPR 2024</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p><a href="https://arxiv.org/abs/2402.07739" class="btn"><strong>Paper</strong></a>
<a href="https://github.com/PierreMarza/task_conditioned_adaptation" class="btn"><strong>Code</strong></a>
<a href="/assets/img/task_conditioned_adaptation/cvpr2024_poster.pdf" class="btn"><strong>CVPR 2024 poster</strong></a>
<a href="https://youtu.be/bqqhnieHz4Q" class="btn"><strong>CVPR 2024 video</strong></a></p>

<p><img src="/assets/img/task_conditioned_adaptation/teaser_gif.gif" width="100%" /></p>

<h2 id="abstract">Abstract</h2>
<p>Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on a wide variety of tasks of the CortexBench benchmark and show that, compared to existing work, it can be addressed with a single policy. In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations.</p>

<iframe width="100%" height="395" src="https://www.youtube.com/embed/bqqhnieHz4Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
</iframe>

<h2 id="task-conditioned-adaptation">Task-conditioned adaptation</h2>
<h3 id="setup">Setup</h3>
<p><img src="/assets/img/task_conditioned_adaptation/tasks_figure.png" width="100%" /></p>

<p>All tasks considered in this work are sequential decision-making problems, where at each discrete timestep \(t\) an agent receives the last 3 visual frames (with height \(h\) and width \(w\)) as an observation \(\mathbf{v}_t \in \mathbb{R}^{3 \times h \times w \times 3}\) and a proprioception input \(\mathbf{p}_t \in \mathbb{R}^{d_a}\), and predicts a continuous action \(\mathbf{\hat{a}}_t \in \mathbb{R}^{d_a}\), where \(d_a\) is the dimension of the action space, which depends on the task at hand. 
We are provided with a training dataset of expert demonstrations to train a single policy, and for inference we study two different setups:</p>

<ul>
  <li>
    <p><strong>Known task</strong>: we <em>a priori</em> know the tasks to be executed. We consider here <strong>12 robotics tasks</strong> from 3 benchmarks, i.e. <strong>Adroit</strong>, <strong>Deepmind control suite</strong> and <strong>MetaWorld</strong>.</p>
  </li>
  <li>
    <p><strong>Few-shot (unknown tasks)</strong>: the trained policy must be adapted to a new unseen task without fine-tuning only given a small set of demonstrations. The ability of our method to adapt to new skills is evaluated on a set of <strong>15 tasks from MetaWorld</strong>.</p>
  </li>
</ul>

<h3 id="base-agent-architecture">Base agent architecture</h3>
<p>Following a large body of work in end-to-end training for robotics, the agent directly maps pixels to actions and decomposes into a visual encoder and a policy. The base visual encoder is a <strong>ViT</strong> model, which has been <strong>pre-trained with masked auto-encoding (MAE)</strong>. More specifically, we keep pre-trained weights from <a href="https://eai-vc.github.io/"><strong>VC-1</strong></a>.</p>

<h3 id="adaptation">Adaptation</h3>
<p>Our contributions are <strong>visual adapter modules</strong> along with a <strong>multi-task policy</strong>, which are all <strong>conditioned on the task at hand</strong>. This is done with a specific <strong>task embedding</strong> for each task, taken from an embedding space, which is aimed to have sufficient regularities to enable few-show generalization to unseen tasks. Importantly, the different adapters and the multi-task policy are conditioned on the same task embedding, leading to a common and shared embedding space.</p>

<h4 id="middle-adapters">Middle adapters</h4>
<p>We add one trainable adapter after each ViT block to modulate its output. We introduce a set of middle adapters, where each adapter is a 2-layer MLP. In the modified visual encoder, each adapter modulates the output of the corresponding self-attention block and is conditioned on the task embedding. The output of a middle adapter is combined with the one of the self-attention layer through a residual connection.</p>

<h4 id="top-adapter">Top adapter</h4>
<p>A top adapter, also conditioned on the task at hand, is added after the ViT model, to transform the output to be fed to the multi-task policy (presented below). It has the same architecture as a single middle adapter.</p>

<h4 id="multi-task-policy">Multi-task policy</h4>
<p>We train a single MLP multi-task policy on the 12 considered known tasks. Its action space is the union of the action spaces of the different tasks. During training we apply a masking procedure on the output, considering only the actions possible for the task at hand.</p>

<h3 id="training">Training</h3>
<p>We train the model by keeping the weights of the pre-trained vision encoder (\(\theta_{\phi}\)) model frozen, only the weights of the adapter modules (\(\theta_A\) and \(\theta_{\tau}\) for middle and top adapters respectively), the multi-task policy, an introduced aggregation layer (\(\theta_{\psi}\) – see paper for more details) and the embedding layer (\(\theta_g\)) predicting the task embedding are trained. We train with imitation learning, more specifically Behavior Cloning (BC): for each known task, we have access to a set of expert trajectories that are composed of discrete steps, including expert actions.</p>
<p align="center">
    <img src="/assets/img/task_conditioned_adaptation/method_figure_train.png" width="80%" />
</p>

<h3 id="few-shot-adaptation-task-embedding-search">Few-shot adaptation (task embedding search)</h3>
<p>For the unknown setting, the task embedding is unknown at inference and needs to be estimated from a set of example demonstrations (sequences of demonstrations and actions). We exploit the conditioning property of the policy itself to estimate the embedding as the one which obtains the highest probability of the demonstration actions, when the policy is applied to the demonstration inputs.</p>
<p align="center">
    <img src="/assets/img/task_conditioned_adaptation/method_figure_few_shot_optim.png" width="80%" />
</p>

<h3 id="inference">Inference</h3>
<p>Policy inference is performed as follows, depending on whether we are addressing a known task or a new unknown task,</p>
<p align="center">
    <img src="/assets/img/task_conditioned_adaptation/method_figure_inference.png" width="80%" />
</p>

<h2 id="quantitative-results">Quantitative results</h2>
<p>All considered models are validated and then tested on two different held-out sets. Please see our paper for more details about our evaluation protocol.</p>

<h3 id="known-tasks">Known tasks</h3>
<h4 id="ablation-study">Ablation study</h4>
<p>We show below the mean test performance over the 12 known tasks of different variants of our model. Adding both middle (Middle ad.) and top (top ad.)  adapters improves performance, as well as conditioning such adapters (C: conditioning / NC: no conditioning) on the task embedding.
<img src="/assets/img/task_conditioned_adaptation/ablation_study.png" width="100%" /></p>

<h4 id="per-task-performance">Per-task performance</h4>
<p>We present the per-task test performance of the multi-task policy with and without adapters, along with single-task policies.
<img src="/assets/img/task_conditioned_adaptation/per_task_performance.png" width="100%" /></p>

<h3 id="few-shot-unknown-tasks">Few-shot (unknown tasks)</h3>
<p>Here is the mean performance on new unknown tasks of our policy with task-conditioned adapters, after task embedding search with 5 demonstrations.
<img src="/assets/img/task_conditioned_adaptation/fewshot_per_task_performance.png" width="100%" /></p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Pierre  Marza.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib">
  </d-bibliography>

</html>
