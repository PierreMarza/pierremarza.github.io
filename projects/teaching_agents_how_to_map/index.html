<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Pierre  Marza


  | Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />  -->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/projects/teaching_agents_how_to_map/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation",
      "description": "IROS 2022",
      "published": "October 24, 2022",
      "authors": [
        
        {
          "author": "Pierre Marza",
          "authorURL": "https://pierremarza.github.io/",
          "affiliations": [
            {
              "name": "INSA Lyon",
              "url": ""
            }
          ]
        },
        
        {
          "author": "LaÃ«titia Matignon",
          "authorURL": "https://perso.liris.cnrs.fr/laetitia.matignon/",
          "affiliations": [
            {
              "name": "UCBL",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Olivier Simonin",
          "authorURL": "http://perso.citi-lab.fr/osimonin/",
          "affiliations": [
            {
              "name": "INSA Lyon",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Christian Wolf",
          "authorURL": "https://chriswolfvision.github.io/www/",
          "affiliations": [
            {
              "name": "Naver Labs Europe",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Pierre</span>   Marza
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/challenges/">
                Challenges
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Research
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation</h1>
        <p>IROS 2022</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p><a href="https://arxiv.org/abs/2107.06011" class="btn"><strong>Paper</strong></a>
<a href="https://github.com/PierreMarza/teaching_agents_how_to_map" class="btn"><strong>Code</strong></a></p>

<p><img src="/assets/img/teaching_agents/teaser_video.gif" width="100%" /></p>

<h2 id="abstract">Abstract</h2>
<p>In the context of visual navigation, the capacity to map a novel environment is necessary for an agent to exploit its observation history in the considered place and efficiently reach known goals. This ability can be associated with spatial reasoning, where an agent is able to perceive spatial relationships and regularities, and discover object characteristics. Recent work introduces learnable policies parametrized by deep neural networks and trained with Reinforcement Learning (RL). In classical RL setups, the capacity to map and reason spatially is learned end-to-end, from reward alone. In this setting, we introduce supplementary supervision in the form of auxiliary tasks designed to favor the emergence of spatial perception capabilities in agents trained for a goal-reaching downstream objective. We show that learning to estimate metrics quantifying the spatial relationships between an agent at a given location and a goal to reach has a high positive impact in Multi-Object Navigation settings. Our method significantly improves the performance of different baseline agents, that either build an explicit or implicit representation of the environment, even matching the performance of incomparable oracle agents taking ground-truth maps as input. A learning-based agent from the literature trained with the proposed auxiliary losses was the winning entry to the Multi-Object Navigation Challenge, part of the CVPR 2021 Embodied AI Workshop.</p>

<iframe width="100%" height="395" src="https://www.youtube.com/embed/rzHZNATBec8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
</iframe>

<h2 id="multi-on">Multi-ON</h2>

<p>We target the Multi-ON task, where an agent is required to reach a sequence of target objects, more precisely coloured cylinders, in a certain order, and which was used for a
recent challenge organized in the context of the CVPR 2021 Embodied AI Workshop. Compared to much easier tasks like PointGoal or (Single) Object Navigation, Multi-ON requires
more difficult reasoning capacities, in particular mapping the position of an object once it has been seen. The following capacities are necessary to ensure optimal performance: (i) mapping the object, i.e. storing it in a suitable latent memory representation; (ii) retrieving this location on request and using it for navigation and planning, including deciding when to retrieve this information, i.e. solving a correspondence problem between sub-goals and memory representation. The agent deals with sequences of objects that are randomly placed in the environment. At each time step, it only knows the class of the next target, which is updated when reached. The episode lasts until either the agent has found all objects in the correct order or the time limit is reached.</p>

<h2 id="baselines">Baselines</h2>
<p>We consider different baseline agents in this work. The differences between them are about the inductive biases given to the policy.</p>

<h3 id="nomap">NoMap</h3>
<p><em>NoMap</em> is a recurrent baseline that takes as input at timetsetp \(t\) the previous action, target object class and RGB-D observation that is encoded using a CNN. This information is concatenated and fed to a GRU unit.
<img src="/assets/img/teaching_agents/NoMap.png" width="100%" /></p>

<h3 id="projneuralmap">ProjNeuralMap</h3>
<p><em>ProjNeuralMap</em> builds a 2M map of the environment by projecting features from the CNN that takes the RGB-D observation as input. The projection is done using depth and camera intrinsics.
<img src="/assets/img/teaching_agents/ProjNeuralMap.png" width="100%" /></p>

<h3 id="oraclemap">OracleMap</h3>
<p><em>OracleMap</em> is an oracle baseline that has access to priviledged information, namely a spatial grid map that spans the whole environment, and that contains occupancy information and target object locations.
<img src="/assets/img/teaching_agents/OracleMap.png" width="100%" /></p>

<h3 id="oracleegomap">OracleEgoMap</h3>
<p><em>OracleEgoMap</em> is another oracle baseline that has access to location of target objects. A difference with <em>OracleMap</em> is that the map is only revealed in places that have already been within the agentâs field of view since the beginning of the episode.
<img src="/assets/img/teaching_agents/OracleEgoMap.png" width="100%" /></p>

<h2 id="teaching-agents-how-to-map">Teaching Agents how to Map</h2>
<p><img src="/assets/img/teaching_agents/fig_architecture_3losses_v2.png" width="100%" /></p>

<p>We introduce auxiliary tasks, additional to the classical RL objectives, and formulated as classification problems, which require the agent to predict information on object appearances, which were in its observation history in the current episode. To this end, the base model is augmented with three classification heads taking as input the contextual representation produced by the GRU unit. It is important to note that these additional classifiers are only used at training time to encourage the learning of spatial reasoning. At inference time, i.e. when deploying the agent on new episodes and/or environments, predictions about already seen targets, their relative direction are distance are not considered. Only the output of the actor is taken into account to select actions to execute.</p>

<p><img src="/assets/img/teaching_agents/tasks_figure.png" width="100%" /></p>

<ul>
  <li>
    <p><strong>Direction</strong>: The agent predicts the relative direction of the target object, only if it has been within its field of view
in the observation history of the episode.</p>
  </li>
  <li>
    <p><strong>Distance</strong>: The second task requires the prediction of the Euclidean distance in the egocentric map between the center box, i.e. position of the agent, and the mean of the
grid boxes containing the target object that was observed during the episode.</p>
  </li>
  <li>
    <p><strong>Observed target</strong>: This third loss favors learning whether the agent has previously encountered the target
object.</p>
  </li>
</ul>

<h2 id="results">Results</h2>
<p>We provide a summary of key results (only on the PPL metrics). Please read our paper for more details.</p>

<h3 id="ablation-study">Ablation study</h3>
<p>Augmenting the vanilla RL supervision signal with only one of the three auwiliary losses alone already brings a boost in performance. When combining <em>direction</em> and <em>distance</em> losses, PPL increases even more. Highest performance is achieved when combining the three axiliary tasks.
<img src="/assets/img/teaching_agents/ablation.png" width="100%" /></p>

<h3 id="comparison-between-baselines">Comparison between baselines</h3>
<p>The simple recurrent <em>NoMap</em> baseline performs surprisingly well when trained with our auxiliary losses, outperforming <em>ProjNeuralMap</em> trained with a vanilla RL objective. However, when augementing the training of <em>ProjNeuralMap</em> with the proposed auxiliary objectives, we reach state-of-the-art performance. Finally, even the <em>OracleEgoMap</em> baseline benefits from the additional supervision signal. As it already has access to some priviledged information, this might suggest the auxiliary losses help to learn to perform spatial reasoning.
<img src="/assets/img/teaching_agents/baselines_comp.png" width="100%" /></p>

<h3 id="qualitative-results">Qualitative results</h3>
<p>Below is a video prsenting a few evaluation episodes.</p>
<iframe width="100%" height="395" src="https://www.youtube.com/embed/syH3nMcZvII" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
</iframe>

<h2 id="multi-on-challenge">Multi-ON Challenge</h2>
<p>The <em>ProjNeuralMap</em> baseline trained with our auxiliary losses was the winning entry to the CVPR 2021 <a href="http://multion-challenge.cs.sfu.ca/2021.html">Multi-ON Challenge</a> part of the <a href="https://embodied-ai.org/cvpr2021">Embodied AI Workshop</a>.</p>

<iframe width="100%" height="395" src="https://www.youtube.com/embed/boDaAORoKho" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
</iframe>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Pierre  Marza.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib">
  </d-bibliography>

</html>
