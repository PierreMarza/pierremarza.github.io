<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Pierre  Marza


  | AutoNeRF: Training Implicit Scene Representations with Autonomous Agents

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />  -->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/projects/autonerf/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "AutoNeRF: Training Implicit Scene Representations with Autonomous Agents",
      "description": "IROS 2024",
      "published": "October 1, 2023",
      "authors": [
        
        {
          "author": "Pierre Marza",
          "authorURL": "https://pierremarza.github.io/",
          "affiliations": [
            {
              "name": "INSA Lyon",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Laëtitia Matignon",
          "authorURL": "https://perso.liris.cnrs.fr/laetitia.matignon/",
          "affiliations": [
            {
              "name": "UCBL",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Olivier Simonin",
          "authorURL": "http://perso.citi-lab.fr/osimonin/",
          "affiliations": [
            {
              "name": "INSA Lyon",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Dhruv Batra",
          "authorURL": "https://faculty.cc.gatech.edu/~dbatra/",
          "affiliations": [
            {
              "name": "Georgia Tech, Meta AI",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Christian Wolf",
          "authorURL": "https://chriswolfvision.github.io/www/",
          "affiliations": [
            {
              "name": "Naver Labs Europe",
              "url": ""
            }
          ]
        },
        
        {
          "author": "Devendra Singh Chaplot",
          "authorURL": "https://devendrachaplot.github.io/",
          "affiliations": [
            {
              "name": "Meta AI",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Pierre</span>   Marza
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/challenges/">
                Challenges
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Research
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>AutoNeRF: Training Implicit Scene Representations with Autonomous Agents</h1>
        <p>IROS 2024</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p><a href="https://arxiv.org/abs/2304.11241" class="btn"><strong>Paper</strong></a>
<a href="https://github.com/PierreMarza/autonerf" class="btn"><strong>Code</strong></a>
<a href="/assets/img/autonerf/nerf4adr_poster.pdf" class="btn"><strong>NeRF4ADR Workshop (ICCV 2023) poster</strong></a></p>

<p><a href="https://youtu.be/CJz2_pAeSKk" class="btn"><strong>IROS 2024 video</strong></a></p>

<h2 id="abstract">Abstract</h2>
<p>Implicit representations such as Neural Radiance Fields (NeRF) have been shown to be very effective at novel view synthesis. However, these models typically require manual and careful human data collection for training. In this paper, we present AutoNeRF, a method to collect data required to train NeRFs using autonomous embodied agents. Our method allows an agent to explore an unseen environment efficiently and use the experience to build an implicit map representation autonomously. We compare the impact of different exploration strategies including handcrafted frontier-based exploration and modular approaches composed of trained high-level planners and classical low-level path followers. We train these models with different reward functions tailored to this problem and evaluate the quality of the learned representations on four different downstream tasks: classical viewpoint rendering, map reconstruction, planning, and pose refinement. Empirical results show that NeRFs can be trained on actively collected data using just a single episode of experience in an unseen environment, and can be used for several downstream robotic tasks, and that modular trained exploration models significantly outperform the classical baselines.</p>

<p><img src="/assets/img/autonerf/teaser_video.gif" width="100%" /></p>

<iframe width="100%" height="395" src="https://www.youtube.com/embed/CJz2_pAeSKk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="">
</iframe>

<h2 id="reconstructing-house-scale-scenes">Reconstructing house-scale scenes</h2>
<p>We start by illustrating the possibility of autonomously reconstructing complex large-scale environments such as apartments or houses from the continuous representations trained on data collected by agents exploring a scene using a modular policy. You can visualize RGB and semantics meshes (extracted from NeRF models) of 5 scenes from the Gibson val set. The semantics head of the NeRF models was trained with GT labels from the Habitat simulator.</p>

<!-- Import the component -->
<script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.0.1/model-viewer.min.js"></script>

<!-- Code adapted from https://modelviewer.dev/examples/augmentedreality/index.html#customButton -->
<model-viewer alt="Collierville" src="/assets/img/autonerf/Corozal_rgb.glb" style="width: 100%; height: 600px; background-color: #404040" poster="/assets/img/autonerf/Corozal_rgb.png" exposure=".8" auto-rotate="" camera-controls="">
  <div class="slider">
    <div class="slides">
      <button class="slide selected" onclick="switchSrc(this, 'Corozal_rgb')" style="background-image: url('/assets/img/autonerf/Corozal_rgb.png');">
      </button>
      <button class="slide" onclick="switchSrc(this, 'Corozal_sem')" style="background-image: url('/assets/img/autonerf/Corozal_sem.png');">
      </button>
      <button class="slide" onclick="switchSrc(this, 'Collierville_rgb')" style="background-image: url('/assets/img/autonerf/Collierville_rgb.png');">
      </button>
      <button class="slide" onclick="switchSrc(this, 'Collierville_sem')" style="background-image: url('/assets/img/autonerf/Collierville_sem.png');">
      </button>
      <button class="slide" onclick="switchSrc(this, 'Darden_rgb')" style="background-image: url('/assets/img/autonerf/Darden_rgb.png');">
      </button>
      <button class="slide" onclick="switchSrc(this, 'Darden_sem')" style="background-image: url('/assets/img/autonerf/Darden_sem.png');">
      </button>
      <button class="slide" onclick="switchSrc(this, 'Markleeville_rgb')" style="background-image: url('/assets/img/autonerf/Markleeville_rgb.png');">
      </button>
      <button class="slide" onclick="switchSrc(this, 'Markleeville_sem')" style="background-image: url('/assets/img/autonerf/Markleeville_sem.png');">
      </button>
      <button class="slide" onclick="switchSrc(this, 'Wiconisco_rgb')" style="background-image: url('/assets/img/autonerf/Wiconisco_rgb.png');">
      </button>
      <button class="slide" onclick="switchSrc(this, 'Wiconisco_sem')" style="background-image: url('/assets/img/autonerf/Wiconisco_sem.png');">
      </button>
    </div>
  </div>
</model-viewer>

<script type="module">
  const modelViewer = document.querySelector("model-viewer");

  window.switchSrc = (element, name) => {
    const base = "/assets/img/autonerf/" + name;
    modelViewer.alt = name;
    modelViewer.src = base + '.glb';
    modelViewer.poster = base + '.png';
    const slides = document.querySelectorAll(".slide");
    slides.forEach((element) => {element.classList.remove("selected");});
    element.classList.add("selected");
  };
</script>

<style>
  /* This keeps child nodes hidden while the element loads */
  :not(:defined) > * {
    display: none;
  }

  .slider {
    width: 100%;
    text-align: center;
    overflow: hidden;
    position: absolute;
    bottom: 16px;
  }

  .slides {
    display: flex;
    overflow-x: auto;
    scroll-snap-type: x mandatory;
    scroll-behavior: smooth;
    -webkit-overflow-scrolling: touch;
  }

  .slide {
    scroll-snap-align: start;
    flex-shrink: 0;
    width: 100px;
    height: 100px;
    background-size: contain;
    background-repeat: no-repeat;
    background-position: center;
    background-color: #fff;
    margin-right: 10px;
    border-radius: 10px;
    border: none;
    display: flex;
  }

  .slide.selected {
    border: 2px solid #4285f4;
  }

  .slide:focus {
    outline: none;
  }

  .slide:focus-visible {
    outline: 1px solid #4285f4;
  }
</style>

<h2 id="exploration-policy">Exploration Policy</h2>
<p>The trained policy aims to allow an agent to explore a 3D scene to collect a sequence of 2D RGB and semantic frames and camera poses, that will be used to train the continuous scene representation. Following previous work, we adapt a modular policy composed of a <em>Mapping</em> process that builds a <em>Semantic Map</em>, a <em>Global Policy</em> that outputs a global waypoint from the semantic map as input, and finally, a <em>Local Policy</em> that navigates towards the global goal.</p>

<p align="center">
    <img src="/assets/img/autonerf/policy_figure.png" width="80%" />
</p>

<h2 id="reward-definitions">Reward definitions</h2>
<p>We consider different reward signals for training the Global Policy tailored to our task of scene reconstruction, and which differ in the importance they give to different aspects of the scene. All these signals are computed in a self-supervised fashion using the metric map representations built by the exploration policy:</p>

<ul>
  <li><strong>Explored area</strong> — <em>Ours (cov.)</em> optimizes the coverage of the scene, i.e. the size of the explored area.</li>
  <li><strong>Obstacle coverage</strong> — <em>Ours (obs.)</em> optimizes the coverage of obstacles in the scene. It targets tasks where obstacles are considered more important than navigable floor space, which is arguably the case when viewing is less important than navigating.</li>
  <li><strong>Semantic object coverage</strong> — <em>Ours (sem.)</em> optimizes the coverage of the semantic classes detected and segmented in the semantic metric map. This reward removes obstacles that are not explicitly identified as a notable semantic class.</li>
  <li><strong>Viewpoints coverage</strong> — <em>Ours (view.)</em> optimizes for the usage of the trained implicit representation as a dense and continuous representation of the scene usable to render arbitrary new viewpoints, either for later visualization as its own downstream task, or for training new agents in simulation. To this end, we propose to maximize coverage not only in terms of agent positions, but also in terms of agent viewpoints. Such reward functions does not only encourage to cover all objects in the scene, but also to view them from different viewpoints.</li>
</ul>

<h2 id="downstream-tasks">Downstream tasks</h2>
<p>Prior work on implicit representations generally focused on two different settings: (i) evaluating the quality of a neural field based on its new view rendering abilities given a dataset of (carefully selected) training views, and (ii) evaluating the quality of a scene representation in robotics conditioned on given (constant) trajectories, evaluated as reconstruction accuracy. We cast this task in a more holistic way and more aligned with our scene understanding objective. We evaluate the impact of trajectory generation (through exploration policies) directly on the quality of the representation, which we evaluate in a goal-oriented way through multiple tasks related to robotics.</p>

<p>We present the different downstream tasks and qualitative results for <em>Ours (obs.)</em>. A quantititative comparison between policies is presented in the paper.</p>

<h3 id="task-1-rendering">Task 1: Rendering</h3>
<p>This task is the closest to the evaluation methodology prevalent in the neural field literature. We evaluate the rendering of RGB frames and semantic segmentation. Unlike the common method of evaluating an implicit representation on a subset of frames within the trajectory, we evaluate it on a set of uniformly sampled camera poses within the scene, independently of the trajectory taken by the policy. This allows us to evaluate the representation of the complete scene and not just its interpolation ability.</p>

<p>Below are rendering examples, where the semantic head of the NeRF model was trained with GT semantic labels from the Habitat simulator. The same NeRF model will be used to provide qualitative examples in the next subsections.</p>
<p align="center">
    <img src="/assets/img/autonerf/viz_rendering_figure_supp_mat.png" width="100%" />
</p>

<h3 id="task-2-metric-map-estimation">Task 2: Metric Map Estimation</h3>
<p>While rendering quality is linked to perception of the scene, it is not necessarily a good indicator of its structural content, which is crucial for robotic downstream tasks. We evaluate the quality of the estimated structure by translating the continuous representation into a format, which is very widely used in map-and-plan baselines for navigation, a binary top-down (bird’s-eye-view=BEV) map storing occupancy and semantic category information and compare it with the ground-truth from the simulator. We evaluate obstacle and semantic maps using accuracy, precision, and recall.</p>
<p align="center">
    <img src="/assets/img/autonerf/viz_map_estimation.png" width="90%" />
</p>

<h3 id="task-3-planning">Task 3: Planning</h3>
<p>Using maps for navigation, it is difficult to pinpoint the exact precision required for successful planning, as certain artifacts and noises might not have a strong impact on reconstruction metrics, but could lead to navigation problems, and vice-versa. We perform goal-oriented evaluation and measure to what extent path planning can be done on the obtained top-down maps.</p>
<p align="center">
    <img src="/assets/img/autonerf/viz_planning.png" width="80%" />
</p>

<h3 id="task-4-pose-refinement">Task 4: Pose Refinement</h3>
<p>This task involves correcting an initial noisy camera position and associated rendered view and optimizing the position until a given ground-truth position is reached, which is given through its associated rendered view only. The optimization process therefore leads to a trajectory in camera pose space. This task is closely linked to visual servoing with a <em>eye-in-hand</em> configuration, a standard problem in robotics, in particular in its <em>direct</em> variant, where the optimization is directly performed over losses on the observed pixel space.</p>
<p align="center">
    <img src="/assets/img/autonerf/camera_pose_refinement_video.gif" width="100%" />
</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Pierre  Marza.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib">
  </d-bibliography>

</html>
