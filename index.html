<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Pierre  Marza


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />  -->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Pierre</span>   Marza
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/challenges/">
                Challenges
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Research
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Pierre</span>  Marza
    </h1>
     <p class="desc">Postdoctoral Researcher - Computer Vision, Deep Learning, Medical Imaging</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/profile_picture.jpg">
      
      
      
      <div class="social">
        <div class="contact-icons">
          <a href="mailto:%70%69%65%72%72%65.%6D%61%72%7A%61@%63%65%6E%74%72%61%6C%65%73%75%70%65%6C%65%63.%66%72"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=NAI5mi4AAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>



<a href="https://www.linkedin.com/in/pierre-marza" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>











        </div>
        <div class="contact-note"></div>
      </div>
      
    </div>
    

    <div class="clearfix">
      <p>I am a Postdoctoral Researcher at <a href="https://www.centralesupelec.fr/">CentraleSupelec</a> in the <a href="https://biomathematics.mics.centralesupelec.fr/">Biomathematics</a> team of the <a href="https://mics.centralesupelec.fr/">MICS</a> lab, studying Computer Vision and Deep Learning for Medical Imaging.</p>

<p>Prior to this, I was a PhD student at <a href="https://www.insa-lyon.fr/en/">INSA Lyon</a>, in the <a href="https://liris.cnrs.fr/en">LIRIS</a> and <a href="https://www.citi-lab.fr/">CITI</a> labs, advised by <a href="https://perso.liris.cnrs.fr/laetitia.matignon/">Laetita Matignon</a>, <a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a> and <a href="https://perso.liris.cnrs.fr/christian.wolf/">Christian Wolf</a>. I studied Visual Navigation, Embodied AI, Spatial Reasoning, more specifically how to learn to represent 3D space, generalize to new environments and master diverse tasks from light supervision. During my PhD, I interned at <a href="https://ai.facebook.com/">Meta AI</a> (FAIR) with <a href="https://devendrachaplot.github.io/">Devendra Chaplot</a> in summer 2022.</p>

<p>A few years ago, I spent some time as an intern at <a href="https://www.noahlab.com.hk/#/home">Huawei Noah’s Ark Computer Vision Lab</a> in London, working on Image Enhancement with <a href="https://sjmoran.github.io/">Sean Moran</a>, <a href="https://smcdonagh.github.io/">Steven McDonagh</a>, <a href="https://parisots.github.io/">Sarah Parisot</a>, <a href="http://www.eecs.qmul.ac.uk/~gslabaugh/">Greg Slabaugh</a>, and others. I also interned at Orange Labs in Rennes, studying Visual Question Answering with <a href="https://corentinkervadec.github.io/">Corentin Kervadec</a>, <a href="https://scholar.google.com/citations?user=CoOz8K0AAAAJ&amp;hl=en">Grigory Antipov</a>, <a href="https://scholar.google.com/citations?user=olfpe-kAAAAJ&amp;hl=fr">Moez Baccouche</a> and <a href="https://perso.liris.cnrs.fr/christian.wolf/">Christian Wolf</a>.</p>

<p>I got my Master’s degree from <a href="https://www.insa-lyon.fr/en/">INSA Lyon</a>, with a major in Computer Science. I spent a semester at <a href="https://www.kth.se/en">KTH</a> in Stockholm, studying Deep Learning, Machine Learning and Reinforcement Learning.</p>

<p><strong>Reviewing experience</strong>: TPAMI, ICML 2022 (outstanding reviewer), ICLR 2023-2024, ICCV 2023, NeurIPS 2023, ECCV 2024 (outstanding reviewer)</p>

<p><strong>My CV is available <a href="/assets/pdf/CV_Pierre_Marza.pdf">here</a></strong></p>

    </div>

    
      <div class="news">
  <h2>News</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Jan 6, 2025</th>
          <td>
            
              I am starting my Postdoc in the <a href="https://biomathematics.mics.centralesupelec.fr/">Biomathematics</a> team of the <a href="https://mics.centralesupelec.fr/">MICS</a> lab at <a href="https://www.centralesupelec.fr/">CentraleSupelec</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Nov 25, 2024</th>
          <td>
            
              I successfully defended my PhD: <a href="https://theses.hal.science/tel-04846767/">Learning spatial representations for single-task navigation and multi-task policies</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 23, 2024</th>
          <td>
            
              I have been selected as an outstanding reviewer for <strong>ECCV 2024</strong>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 30, 2024</th>
          <td>
            
              One paper accepted at <strong>IROS 2024</strong>: <a href="https://arxiv.org/abs/2304.11241">AutoNeRF: Training Implicit Scene Representations with Autonomous Agents</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Feb 27, 2024</th>
          <td>
            
              One paper accepted at <strong>CVPR 2024</strong>: <a href="https://arxiv.org/abs/2402.07739">Task-conditioned adaptation of visual features in multi-task policy learning</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 18, 2023</th>
          <td>
            
              I will be presenting 2 papers at the <a href="https://neural-fields.xyz/"><strong>Nerf4ADR workshop</strong></a> at <strong>ICCV 2023</strong>: <a href="https://arxiv.org/abs/2210.05129">Multi-Object Navigation with dynamically learned neural implicit representations</a> and <a href="https://arxiv.org/abs/2304.11241">AutoNeRF: Training Implicit Scene Representations with Autonomous Agents</a> (<strong>oral presentation</strong>).

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 14, 2023</th>
          <td>
            
              One paper accepted at <strong>ICCV 2023</strong>: <a href="https://arxiv.org/abs/2210.05129">Multi-Object Navigation with dynamically learned neural implicit representations</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 9, 2023</th>
          <td>
            
              I am attending the <a href="https://iplab.dmi.unict.it/icvss2023/Home">International Computer Vision Summer School (ICVSS 2023)</a> in Sicily.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Apr 25, 2023</th>
          <td>
            
              New paper on arXiv: <a href="https://arxiv.org/abs/2304.11241">AutoNeRF: Training Implicit Scene Representations with Autonomous Agents</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 11, 2022</th>
          <td>
            
              New paper on arXiv: <a href="https://arxiv.org/abs/2210.05129">Multi-Object Navigation with dynamically learned neural implicit representations</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 17, 2022</th>
          <td>
            
              I have been selected as an outstanding reviewer (Top 10%) for <strong>ICML 2022</strong>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 29, 2022</th>
          <td>
            
              One paper accepted at <strong>IROS 2022</strong>: <a href="https://arxiv.org/abs/2107.06011">Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 27, 2022</th>
          <td>
            
              Beginning of my internship at <a href="https://ai.facebook.com/">Meta AI</a> (FAIR) in Menlo Park, working with <a href="https://devendrachaplot.github.io/">Devendra Chaplot</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Feb 14, 2022</th>
          <td>
            
              New paper on arXiv: <a href="https://arxiv.org/abs/2202.06858">An experimental study of the vision-bottleneck in VQA</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 29, 2021</th>
          <td>
            
              I presented some of our work on Learning Spatial Reasoning for Multi-Object Navigation during a Workshop on Learning and Robotics organized by the GDR ISIS, and entitled <a href="http://www.gdr-isis.fr/index.php/reunion/456/">Apprentissage et Robotique</a>. Slides are available <a href="/assets/pdf/GDR_ISIS_29_06_2021_Pierre_Marza.pdf">here</a>.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>Selected publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">PhD thesis</abbr>
    
  
  </div>

  <div id="marza2024learning" class="col-sm-8">
    
      <div class="title">Learning spatial representations for single-task navigation and multi-task policies</div>
      <div class="author">
        
          
          
          
          
          
          
            
              <em>Pierre Marza</em>
            
          
        
      </div>

      <div class="periodical">
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://theses.hal.science/tel-04846767/" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Autonomously behaving in the 3D world requires a large set of skills, among which are perceiving the surrounding environment, representing it precisely and efficiently enough to keep track of the past, making decisions and acting to achieve specified goals. Animals, for instance humans, stand out by their robustness when it comes to acting in the world. In particular, they can efficiently generalize to new environments, but are also able to rapidly master many tasks of interest from a few examples. This manuscript will study how artificial neural networks can be trained to acquire a subset of these abilities. We will first focus on training neural agents to perform semantic mapping, both from augmented supervision signal and with proposed neural-based scene representations. Neural agents are often trained with Reinforcement Learning (RL) from a sparse reward signal. Guiding the learning of scene mapping abilities by augmenting the vanilla RL supervision signal with auxiliary spatial reasoning tasks will help navigating efficiently. Instead of modifying the training signal of neural agents, we will also see how incorporating specific neural-based representations of semantics and geometry within the architecture of the agent can help improve performance in goal-driven navigation. Then, we will study how to best explore a 3D environment in order to build neural representations of space that are as satisfying as possible based on robotic-oriented metrics we will propose. Finally, we will move from navigation-only to multi-task agents, and see how important it is to tailor visual features from sensor observations to the task at hand to perform a wide variety of tasks, but also to adapt to new unknown tasks from a few demonstrations. This manuscript will thus address different important questions such as: How to represent a 3D scene and keep track of previous experience in an environment? – How to robustly adapt to new environments, scenarios, and potentially new tasks? – How to train agents on long-horizon sequential tasks? – How to jointly master all required sub-skills? – What is the importance of perception in robotics?</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">marza2024learning</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{PhD thesis}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning spatial representations for single-task navigation and multi-task policies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marza, Pierre}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{INSA Lyon}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://theses.hal.science/tel-04846767/}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
    
  
  </div>

  <div id="marza2024task" class="col-sm-8">
    
      <div class="title">Task-conditioned adaptation of visual features in multi-task policy learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Pierre Marza</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Laetitia Matignon,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Olivier Simonin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Christian Wolf
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://pierremarza.github.io/projects/task_conditioned_adaptation/" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
    
    
      
      <a href="https://arxiv.org/abs/2402.07739" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/PierreMarza/task_conditioned_adaptation" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on a wide variety of tasks of the CortexBench benchmark and show that, compared to existing work, it can be addressed with a single policy. In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">marza2024task</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Task-conditioned adaptation of visual features in multi-task policy learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marza, Pierre and Matignon, Laetitia and Simonin, Olivier and Wolf, Christian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2402.07739}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://pierremarza.github.io/projects/task_conditioned_adaptation/}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/PierreMarza/task_conditioned_adaptation}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IROS</abbr>
    
  
  </div>

  <div id="marza2023autonerf" class="col-sm-8">
    
      <div class="title">AutoNeRF: Training Implicit Scene Representations with Autonomous Agents</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Pierre Marza</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Laetitia Matignon,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Olivier Simonin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Dhruv Batra,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Christian Wolf,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Devendra Singh Chaplot
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Intelligent Robots and Systems (IROS)</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://pierremarza.github.io/projects/autonerf/" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
    
    
      
      <a href="https://arxiv.org/abs/2304.11241" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/PierreMarza/autonerf" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Implicit representations such as Neural Radiance Fields (NeRF) have been shown to be very effective at novel view synthesis. However, these models typically require manual and careful human data collection for training. In this paper, we present AutoNeRF, a method to collect data required to train NeRFs using autonomous embodied agents. Our method allows an agent to explore an unseen environment efficiently and use the experience to build an implicit map representation autonomously. We compare the impact of different exploration strategies including handcrafted frontier-based exploration and modular approaches composed of trained high-level planners and classical low-level path followers. We train these models with different reward functions tailored to this problem and evaluate the quality of the learned representations on four different downstream tasks: classical viewpoint rendering, map reconstruction, planning, and pose refinement. Empirical results show that NeRFs can be trained on actively collected data using just a single episode of experience in an unseen environment, and can be used for several downstream robotic tasks, and that modular trained exploration models significantly outperform the classical baselines.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">marza2023autonerf</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IROS}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{AutoNeRF: Training Implicit Scene Representations with Autonomous Agents}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marza, Pierre and Matignon, Laetitia and Simonin, Olivier and Batra, Dhruv and Wolf, Christian and Chaplot, Devendra Singh}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2304.11241}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://pierremarza.github.io/projects/autonerf/}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/PierreMarza/autonerf}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICCV</abbr>
    
  
  </div>

  <div id="marza2023dynamic_impl_repr" class="col-sm-8">
    
      <div class="title">Multi-Object Navigation with dynamically learned neural implicit representations</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Pierre Marza</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Laetitia Matignon,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Olivier Simonin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Christian Wolf
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Computer Vision (ICCV)</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://pierremarza.github.io/projects/dynamic_implicit_representations/" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
    
    
      
      <a href="https://arxiv.org/abs/2210.05129" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/PierreMarza/dynamic_implicit_representations" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Understanding and mapping a new environment are core abilities of any autonomously navigating agent. While classical robotics usually estimates maps in a stand-alone manner with SLAM variants, which maintain a topological or metric representation, end-to-end learning of navigation keeps some form of memory in a neural network. Networks are typically imbued with inductive biases, which can range from vectorial representations to birds-eye metric tensors or topological structures. In this work, we propose to structure neural networks with two neural implicit representations, which are learned dynamically during each episode and map the content of the scene: (i) the Semantic Finder predicts the position of a previously seen queried object; (ii) the Occupancy and Exploration Implicit Representation encapsulates information about explored area and obstacles, and is queried with a novel global read mechanism which directly maps from function space to a usable embedding space. Both representations are leveraged by an agent trained with Reinforcement Learning (RL) and learned online during each episode. We evaluate the agent on Multi-Object Navigation and show the high impact of using neural implicit representations as a memory source.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">marza2023dynamic_impl_repr</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICCV}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-Object Navigation with dynamically learned neural implicit representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marza, Pierre and Matignon, Laetitia and Simonin, Olivier and Wolf, Christian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://pierremarza.github.io/projects/dynamic_implicit_representations/}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2210.05129}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/PierreMarza/dynamic_implicit_representations}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IROS</abbr>
    
  
  </div>

  <div id="marza2022teaching" class="col-sm-8">
    
      <div class="title">Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation (Winning entry of the MultiON Challenge at CVPR 2021)</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Pierre Marza</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Laëtitia Matignon,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Olivier Simonin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Christian Wolf
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Intelligent Robots and Systems (IROS)</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://pierremarza.github.io/projects/teaching_agents_how_to_map/" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
    
    
      
      <a href="https://arxiv.org/abs/2107.06011" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/PierreMarza/teaching_agents_how_to_map" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In the context of visual navigation, the capacity to map a novel environment is necessary for an agent to exploit its observation history in the considered place and efficiently reach known goals. This ability can be associated with spatial reasoning, where an agent is able to perceive spatial relationships and regularities, and discover object affordances. In classical Reinforcement Learning (RL) setups, this capacity is learned from reward alone. We introduce supplementary supervision in the form of auxiliary tasks designed to favor the emergence of spatial perception capabilities in agents trained for a goal-reaching downstream objective. We show that learning to estimate metrics quantifying the spatial relationships between an agent at a given location and a goal to reach has a high positive impact in Multi-Object Navigation settings. Our method significantly improves the performance of different baseline agents, that either build an explicit or implicit representation of the environment, even matching the performance of incomparable oracle agents taking ground-truth maps as input.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">marza2022teaching</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{IROS}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marza, Pierre and Matignon, La{\"{e}}titia and Simonin, Olivier and Wolf, Christian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation (Winning entry of the MultiON Challenge at CVPR 2021)}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://pierremarza.github.io/projects/teaching_agents_how_to_map/}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2107.06011}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/PierreMarza/teaching_agents_how_to_map}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
    
  
  </div>

  <div id="moran2020deeplpf" class="col-sm-8">
    
      <div class="title">DeepLPF: Deep Local Parametric Filters for Image Enhancement</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Sean Moran,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Pierre Marza</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Steven McDonagh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sarah Parisot,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gregory Slabaugh
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Moran_DeepLPF_Deep_Local_Parametric_Filters_for_Image_Enhancement_CVPR_2020_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/sjmoran/DeepLPF" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Digital artists often improve the aesthetic quality of digital photographs through manual retouching. Beyond global adjustments, professional image editing programs provide local adjustment tools operating on specific parts of an image. Options include parametric (graduated, radial filters) and unconstrained brush tools. These highly expressive tools enable a diverse set of local image enhancements. However, their use can be time consuming, and requires artistic capability. State-of-the-art automated image enhancement approaches typically focus on learning pixel-level or global enhancements. The former can be noisy and lack interpretability, while the latter can fail to capture fine-grained adjustments. In this paper, we introduce a novel approach to automatically enhance images using learned spatially local filters of three different types (Elliptical Filter, Graduated Filter, Polynomial Filter). We introduce a deep neural network, dubbed Deep Local Parametric Filters (DeepLPF), which regresses the parameters of these spatially localized filters that are then automatically applied to enhance the image. DeepLPF provides a natural form of model regularization and enables interpretable, intuitive adjustments that lead to visually pleasing results. We report on multiple benchmarks and show that DeepLPF produces state-of-the-art performance on two variants of the MIT-Adobe 5k dataset, often using a fraction of the parameters required for competing methods.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">moran2020deeplpf</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Moran, Sean and Marza, Pierre and McDonagh, Steven and Parisot, Sarah and Slabaugh, Gregory}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DeepLPF: Deep Local Parametric Filters for Image Enhancement}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://openaccess.thecvf.com/content_CVPR_2020/html/Moran_DeepLPF_Deep_Local_Parametric_Filters_for_Image_Enhancement_CVPR_2020_paper.html}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/sjmoran/DeepLPF}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
      <div class="publications">
    <h2>Patents</h2>
    <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Patent</abbr>
    
  
  </div>

  <div id="moran2021patent" class="col-sm-8">
    
      <div class="title">A device and method for image processing</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Sean Moran,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Pierre Marza</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Steven McDonagh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sarah Parisot,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gregory Slabaugh
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
    
    
    
    
      
      <a href="https://patentimages.storage.googleapis.com/00/f5/06/cc3f0f942174df/WO2021093956A1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
  </div>
  
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Pierre  Marza.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
