<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Pierre  Marza


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />  -->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Pierre</span>   Marza
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/challenges/">
                Challenges
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Research
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Pierre</span>  Marza
    </h1>
     <p class="desc">Postdoctoral Researcher - Computer Vision, Deep Learning, Medical Imaging</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/profile_picture.jpg">
      
      
      
      <div class="social">
        <div class="contact-icons">
          <a href="mailto:%70%69%65%72%72%65.%6D%61%72%7A%61@%63%65%6E%74%72%61%6C%65%73%75%70%65%6C%65%63.%66%72"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=NAI5mi4AAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/PierreMarza" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/pierre-marza" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>











        </div>
        <div class="contact-note"></div>
      </div>
      
    </div>
    

    <div class="clearfix">
      <p>I am a Postdoctoral Researcher at <a href="https://www.centralesupelec.fr/">CentraleSupelec</a> in the <a href="https://biomathematics.mics.centralesupelec.fr/">Biomathematics</a> team of the <a href="https://mics.centralesupelec.fr/">MICS</a> lab, studying Computer Vision and Deep Learning for Medical Imaging.</p>

<p>Prior to this, I was a PhD student at <a href="https://www.insa-lyon.fr/en/">INSA Lyon</a>, in the <a href="https://liris.cnrs.fr/en">LIRIS</a> and <a href="https://www.citi-lab.fr/">CITI</a> labs, advised by <a href="https://perso.liris.cnrs.fr/laetitia.matignon/">Laetita Matignon</a>, <a href="http://perso.citi-lab.fr/osimonin/">Olivier Simonin</a> and <a href="https://perso.liris.cnrs.fr/christian.wolf/">Christian Wolf</a>. I studied Visual Navigation, Embodied AI, Spatial Reasoning, more specifically how to learn to represent 3D space, generalize to new environments and master diverse tasks from light supervision. During my PhD, I interned at <a href="https://ai.facebook.com/">Meta AI</a> (FAIR) with <a href="https://devendrachaplot.github.io/">Devendra Chaplot</a> in summer 2022.</p>

<p>A few years ago, I spent some time as an intern at <a href="https://www.noahlab.com.hk/#/home">Huawei Noah’s Ark Computer Vision Lab</a> in London, working on Image Enhancement with <a href="https://sjmoran.github.io/">Sean Moran</a>, <a href="https://smcdonagh.github.io/">Steven McDonagh</a>, <a href="https://parisots.github.io/">Sarah Parisot</a>, <a href="http://www.eecs.qmul.ac.uk/~gslabaugh/">Greg Slabaugh</a>, and others. I also interned at Orange Labs in Rennes, studying Visual Question Answering with <a href="https://corentinkervadec.github.io/">Corentin Kervadec</a>, <a href="https://scholar.google.com/citations?user=CoOz8K0AAAAJ&amp;hl=en">Grigory Antipov</a>, <a href="https://scholar.google.com/citations?user=olfpe-kAAAAJ&amp;hl=fr">Moez Baccouche</a> and <a href="https://perso.liris.cnrs.fr/christian.wolf/">Christian Wolf</a>.</p>

<p>I got my Master’s degree from <a href="https://www.insa-lyon.fr/en/">INSA Lyon</a>, with a major in Computer Science. I spent a semester at <a href="https://www.kth.se/en">KTH</a> in Stockholm, studying Deep Learning, Machine Learning and Reinforcement Learning.</p>

<p><strong>Reviewing experience</strong>: TPAMI; ICML 2022 (outstanding reviewer); ICLR 2023-2025; ICCV 2023, 2025; NeurIPS 2023, 2025 (top reviewer); ECCV 2024 (outstanding reviewer), CVPR 2025.</p>

<p><strong>My CV is available <a href="/assets/pdf/CV_Pierre_Marza.pdf">here</a></strong></p>

    </div>

    
      <div class="news">
  <h2>News</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Oct 17, 2025</th>
          <td>
            
              I have been selected as a top reviewer for <strong>NeurIPS 2025</strong>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 18, 2025</th>
          <td>
            
              One paper accepted at <strong>NeurIPS 2025</strong> D&amp;B Track as a <strong>Spotlight</strong> presentation: <a href="https://arxiv.org/abs/2507.07860">THUNDER: Tile-level Histopathology image UNDERstanding benchmark
</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 25, 2025</th>
          <td>
            
              One paper accepted at <strong>ICCV 2025</strong>: <a href="https://arxiv.org/abs/2508.14588">Controllable Latent Space Augmentation for Digital Pathology</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 22, 2025</th>
          <td>
            
              New paper on arXiv: <a href="https://arxiv.org/abs/2506.18164">CDG-MAE: Learning Correspondences from Diffusion Generated Views</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jan 6, 2025</th>
          <td>
            
              I am starting my Postdoc in the <a href="https://biomathematics.mics.centralesupelec.fr/">Biomathematics</a> team of the <a href="https://mics.centralesupelec.fr/">MICS</a> lab at <a href="https://www.centralesupelec.fr/">CentraleSupelec</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Nov 25, 2024</th>
          <td>
            
              I successfully defended my PhD: <a href="https://theses.hal.science/tel-04846767/">Learning spatial representations for single-task navigation and multi-task policies</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 23, 2024</th>
          <td>
            
              I have been selected as an outstanding reviewer for <strong>ECCV 2024</strong>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 30, 2024</th>
          <td>
            
              One paper accepted at <strong>IROS 2024</strong>: <a href="https://arxiv.org/abs/2304.11241">AutoNeRF: Training Implicit Scene Representations with Autonomous Agents</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Feb 27, 2024</th>
          <td>
            
              One paper accepted at <strong>CVPR 2024</strong>: <a href="https://arxiv.org/abs/2402.07739">Task-conditioned adaptation of visual features in multi-task policy learning</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 18, 2023</th>
          <td>
            
              I will be presenting 2 papers at the <a href="https://neural-fields.xyz/"><strong>Nerf4ADR workshop</strong></a> at <strong>ICCV 2023</strong>: <a href="https://arxiv.org/abs/2210.05129">Multi-Object Navigation with dynamically learned neural implicit representations</a> and <a href="https://arxiv.org/abs/2304.11241">AutoNeRF: Training Implicit Scene Representations with Autonomous Agents</a> (<strong>oral presentation</strong>).

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 14, 2023</th>
          <td>
            
              One paper accepted at <strong>ICCV 2023</strong>: <a href="https://arxiv.org/abs/2210.05129">Multi-Object Navigation with dynamically learned neural implicit representations</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 9, 2023</th>
          <td>
            
              I am attending the <a href="https://iplab.dmi.unict.it/icvss2023/Home">International Computer Vision Summer School (ICVSS 2023)</a> in Sicily.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Apr 25, 2023</th>
          <td>
            
              New paper on arXiv: <a href="https://arxiv.org/abs/2304.11241">AutoNeRF: Training Implicit Scene Representations with Autonomous Agents</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 11, 2022</th>
          <td>
            
              New paper on arXiv: <a href="https://arxiv.org/abs/2210.05129">Multi-Object Navigation with dynamically learned neural implicit representations</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 17, 2022</th>
          <td>
            
              I have been selected as an outstanding reviewer (Top 10%) for <strong>ICML 2022</strong>.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>Selected publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS DB Spot.</abbr>
    
  
  </div>

  <div id="marza2025thunder" class="col-sm-8">
    
      <div class="title">THUNDER: Tile-level Histopathology image UNDERstanding benchmark</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Pierre Marza</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Leo Fillioux,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sofiène Boutaj,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kunal Mahatha,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Christian Desrosiers,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Pablo Piantanida,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jose Dolz,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Stergios Christodoulidis,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Maria Vakalopoulou
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Neural Information Processing Systems (NeurIPS) D&B Track (Spotlight)</em>
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://mics-lab.github.io/thunder/" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
    
    
      
      <a href="https://arxiv.org/abs/2507.07860" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/MICS-Lab/thunder" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Progress in a research field can be hard to assess, in particular when many concurrent methods are proposed in a short period of time. This is the case in digital pathology, where many foundation models have been released recently to serve as feature extractors for tile-level images, being used in a variety of downstream tasks, both for tile- and slide-level problems. Benchmarking available methods then becomes paramount to get a clearer view of the research landscape. In particular, in critical domains such as healthcare, a benchmark should not only focus on evaluating downstream performance, but also provide insights about the main differences between methods, and importantly, further consider uncertainty and robustness to ensure a reliable usage of proposed models. For these reasons, we introduce THUNDER, a tile-level benchmark for digital pathology foundation models, allowing for efficient comparison of many models on diverse datasets with a series of downstream tasks, studying their feature spaces and assessing the robustness and uncertainty of predictions informed by their embeddings. THUNDER is a fast, easy-to-use, dynamic benchmark that can already support a large variety of state-of-the-art foundation, as well as local user-defined models for direct tile-based comparison. In this paper, we provide a comprehensive comparison of 23 foundation models on 16 different datasets covering diverse tasks, feature analysis, and robustness. The code for THUNDER is publicly available at https://github.com/MICS-Lab/thunder.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">marza2025thunder</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{NeurIPS DB Spot.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{THUNDER}: Tile-level Histopathology image UNDERstanding benchmark}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marza, Pierre and Fillioux, Leo and Boutaj, Sofi{\`e}ne and Mahatha, Kunal and Desrosiers, Christian and Piantanida, Pablo and Dolz, Jose and Christodoulidis, Stergios and Vakalopoulou, Maria}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Neural Information Processing Systems (NeurIPS) D&amp;B Track (Spotlight)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2507.07860}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://mics-lab.github.io/thunder/}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MICS-Lab/thunder}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICCV</abbr>
    
  
  </div>

  <div id="boutaj2025controllable" class="col-sm-8">
    
      <div class="title">Controllable Latent Space Augmentation for Digital Pathology</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Sofiène Boutaj,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Marin Scalbert,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Pierre Marza</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Florent Couzinie-Devy,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Maria Vakalopoulou,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Stergios Christodoulidis
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Computer Vision (ICCV)</em>
      
      
        2025
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://arxiv.org/abs/2508.14588" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/MICS-Lab/HistAug" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Whole slide image (WSI) analysis in digital pathology presents unique challenges due to the gigapixel resolution of WSIs and the scarcity of dense supervision signals. While Multiple Instance Learning (MIL) is a natural fit for slidelevel tasks, training robust models requires large and diverse datasets. Even though image augmentation techniques could be utilized to increase data variability and reduce overfitting, implementing them effectively is not a trivial task. Traditional patch-level augmentation is prohibitively expensive due to the large number of patches extracted from each WSI, and existing feature-level augmentation methods lack control over transformation semantics. We introduce HistAug, a fast and efficient generative model for controllable augmentations in the latent space for digital pathology. By conditioning on explicit patch-level transformations (e.g., hue, erosion), HistAug generates realistic augmented embeddings while preserving initial semantic information. Our method allows the processing of a large number of patches in a single forward pass efficiently, while at the same time consistently improving MIL model performance. Experiments across multiple slide-level tasks and diverse organs show that HistAug outperforms existing methods, particularly in low-data regimes. Ablation studies confirm the benefits of learned transformations over noise-based perturbations and highlight the importance of uniform WSI-wise augmentation. Code is available at https://github.com/MICS-Lab/HistAug.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">boutaj2025controllable</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICCV}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Controllable Latent Space Augmentation for Digital Pathology}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Boutaj, Sofi{\`e}ne and Scalbert, Marin and Marza, Pierre and Couzinie-Devy, Florent and Vakalopoulou, Maria and Christodoulidis, Stergios}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2508.14588}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/MICS-Lab/HistAug}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">PhD thesis</abbr>
    
  
  </div>

  <div id="marza2024learning" class="col-sm-8">
    
      <div class="title">Learning spatial representations for single-task navigation and multi-task policies</div>
      <div class="author">
        
          
          
          
          
          
          
            
              <em>Pierre Marza</em>
            
          
        
      </div>

      <div class="periodical">
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
    
      
      <a href="https://theses.hal.science/tel-04846767/" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Autonomously behaving in the 3D world requires a large set of skills, among which are perceiving the surrounding environment, representing it precisely and efficiently enough to keep track of the past, making decisions and acting to achieve specified goals. Animals, for instance humans, stand out by their robustness when it comes to acting in the world. In particular, they can efficiently generalize to new environments, but are also able to rapidly master many tasks of interest from a few examples. This manuscript will study how artificial neural networks can be trained to acquire a subset of these abilities. We will first focus on training neural agents to perform semantic mapping, both from augmented supervision signal and with proposed neural-based scene representations. Neural agents are often trained with Reinforcement Learning (RL) from a sparse reward signal. Guiding the learning of scene mapping abilities by augmenting the vanilla RL supervision signal with auxiliary spatial reasoning tasks will help navigating efficiently. Instead of modifying the training signal of neural agents, we will also see how incorporating specific neural-based representations of semantics and geometry within the architecture of the agent can help improve performance in goal-driven navigation. Then, we will study how to best explore a 3D environment in order to build neural representations of space that are as satisfying as possible based on robotic-oriented metrics we will propose. Finally, we will move from navigation-only to multi-task agents, and see how important it is to tailor visual features from sensor observations to the task at hand to perform a wide variety of tasks, but also to adapt to new unknown tasks from a few demonstrations. This manuscript will thus address different important questions such as: How to represent a 3D scene and keep track of previous experience in an environment? – How to robustly adapt to new environments, scenarios, and potentially new tasks? – How to train agents on long-horizon sequential tasks? – How to jointly master all required sub-skills? – What is the importance of perception in robotics?</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">marza2024learning</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{PhD thesis}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning spatial representations for single-task navigation and multi-task policies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marza, Pierre}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{INSA Lyon}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://theses.hal.science/tel-04846767/}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CVPR</abbr>
    
  
  </div>

  <div id="marza2024task" class="col-sm-8">
    
      <div class="title">Task-conditioned adaptation of visual features in multi-task policy learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Pierre Marza</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Laetitia Matignon,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Olivier Simonin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Christian Wolf
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        2024
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://pierremarza.github.io/projects/task_conditioned_adaptation/" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
    
    
      
      <a href="https://arxiv.org/abs/2402.07739" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a href="https://github.com/PierreMarza/task_conditioned_adaptation" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on a wide variety of tasks of the CortexBench benchmark and show that, compared to existing work, it can be addressed with a single policy. In particular, we demonstrate that adapting visual features is a key design choice and that the method generalizes to unseen tasks given visual demonstrations.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">marza2024task</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Task-conditioned adaptation of visual features in multi-task policy learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marza, Pierre and Matignon, Laetitia and Simonin, Olivier and Wolf, Christian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2402.07739}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://pierremarza.github.io/projects/task_conditioned_adaptation/}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/PierreMarza/task_conditioned_adaptation}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
      <div class="publications">
    <h2>Patents</h2>
    <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Patent</abbr>
    
  
  </div>

  <div id="moran2021patent" class="col-sm-8">
    
      <div class="title">A device and method for image processing</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Sean Moran,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Pierre Marza</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Steven McDonagh,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sarah Parisot,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gregory Slabaugh
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em></em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
    
    
    
    
      
      <a href="https://patentimages.storage.googleapis.com/00/f5/06/cc3f0f942174df/WO2021093956A1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
  </div>
  
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Pierre  Marza.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
