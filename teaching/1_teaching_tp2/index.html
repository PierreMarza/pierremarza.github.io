<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Pierre  Marza


  | Introduction to Deep Learning - Part 2

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />  -->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/teaching/1_teaching_tp2/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Introduction to Deep Learning - Part 2",
      "description": "Deep Q-Learning",
      "published": "January 4, 2021",
      "authors": [
        
        {
          "author": "Pierre Marza",
          "authorURL": "https://pierremarza.github.io/",
          "affiliations": [
            {
              "name": "INSA, Lyon",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Pierre</span>   Marza
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/challenges/">
                Challenges
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Research
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Introduction to Deep Learning - Part 2</h1>
        <p>Deep Q-Learning</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <h2 id="context">Context</h2>
<p>This session is an introduction to Reinforcement Learning, and more particularly Deep Q-Learning. Some parts of this course, as well as code snippets, are reproduced from this great <a href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html">Pytorch Tutorial</a></p>

<h2 id="installation">Installation</h2>
<p>We will be coding with <strong>Python3</strong> and will use the <a href="https://pytorch.org/"><strong>Pytorch</strong></a> and <a href="https://gym.openai.com/"><strong>gym</strong></a> libraries.</p>

<p>To install Pytorch on your local machine, follow this <a href="https://pytorch.org/get-started/locally/">link</a>.</p>

<p>To install gym, simply run <strong>pip install gym</strong></p>

<h2 id="the-cartpole-task">The Cartpole task</h2>
<p>In the <a href="https://gym.openai.com/envs/CartPole-v1/">Gym Cartpole task</a>, an agent must decide between two actions, i.e. moving the cart to the left or the right, so that the attached pole stays upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.</p>

<p>The first step is to <strong>create a gym environment</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">gym</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">CartPole-v0</span><span class="sh">'</span><span class="p">).</span><span class="n">unwrapped</span>
</code></pre></div></div>

<p>With the following code, you can visualize a random agent. You will notice that episodes obviously end very quickly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="c1"># Reset environment
</span>    <span class="n">initial</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">env</span><span class="p">.</span><span class="nf">render</span><span class="p">()</span>
        <span class="n">random_action</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">random_action</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="basics">Basics</h2>
<p>You have covered the fundamentals of Reinforcement Learning (RL) and Q-Learning in your last <a href="https://chriswolfvision.github.io/www/teaching/deeplearning/cm-deeplearning-5-1-RL.pdf">lecture</a>.</p>

<p><img src="/assets/img/rl.pbm" alt="Alt" title="Drawing from Sutton and Barto, Reinforcement Learning: An Introduction, 1998" /></p>

<h3 id="markov-decision-process-mdp">Markov Decision Process (MDP)</h3>
<p>Any RL problem can be formulated as a <strong>Markov Decision Process</strong> (MDP) characterised by:</p>

<ul>
  <li>Set of states $S$</li>
  <li>Set of actions $A$</li>
  <li>Transition function $P(s_{t+1} \mid s_t, a_t)$</li>
  <li>Reward function $R(s_t, a_t, s_{t+1})$</li>
  <li>Start state $s_0$</li>
  <li>Discount factor $\gamma$</li>
  <li>Horizon $H$</li>
</ul>

<p>A <strong>trajectory $\tau$</strong> is a sequence of states and actions,</p>

\[\tau = (s_0, a_0, s_1, a_1, ..., s_H)\]

<p>We can then define the <strong>return $R(\tau)$</strong> as follows,</p>

\[R(\tau)=\sum_{t=0}^{H} \gamma^{t} r_{t}\]

<p>The goal is to find the <strong>policy $\pi$ maximizing the expected return $J(\pi)$</strong> defined as,</p>

\[J(\pi)=\underset{\tau \sim \pi}{\mathrm{E}}[R(\tau)]\]

<h3 id="q-value">Q-Value</h3>

<p>The optimal Q-Value $Q^*(s, a)$ is the <strong>expected return when starting in state $s$, taking action $a$ and then acting optimally</strong> until the end of the episode. The optimal Q-Value can be defined recursively through the Bellman Equation,</p>

\[Q^{*}(s, a)=\sum_{s^{\prime}} P\left(s^{\prime} \mid s, a\right)\left(R\left(s, a, s^{\prime}\right)+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right)\]

<h3 id="tabular-q-value-iteration">Tabular Q-Value iteration</h3>
<p>When knowing the dynamics of the environment (i.e. $P(s_{t+1} \mid s_t, a_t)$), the optimal policy $\pi^*$ can be found using exact methods such as Q-Value iteration or Policy Iteration. The update rule, repeated until convergence, of the <strong>tabular Q-Value iteration</strong> algorithm is as follows,</p>

\[Q_{k+1}(s, a) \leftarrow \sum_{s^{\prime}} P\left(s^{\prime} \mid s, a\right)\left(R\left(s, a, s^{\prime}\right)+\gamma \max _{a^{\prime}} Q_{k}\left(s^{\prime}, a^{\prime}\right)\right)\]

<h3 id="tabular-q-learning">Tabular Q-Learning</h3>
<p>However, in most interesting problems, such dynamics are unknown. The previous expression can be re-written as an expectation,</p>

\[Q_{k+1}(s, a) \leftarrow \mathbb{E}_{s^{\prime} \sim P\left(s^{\prime} \mid s, a\right)}\left[R\left(s, a, s^{\prime}\right)+\gamma \max _{a^{\prime}} Q_{k}\left(s^{\prime}, a^{\prime}\right)\right]\]

<p>This expectation can be <strong>approximated by sampling</strong>. An agent can thus collect samples so that we can approximate $Q_{k+1}(s, a)$ instead of computing it exactly. Given a new collected sample state $s^{\prime}$, the Q-Learning update equation becomes,</p>

\[Q_{k+1}(s, a) \leftarrow(1-\alpha) Q_{k}(s, a)+\alpha\left[R\left(s, a, s^{\prime}\right)+\gamma \max _{a^{\prime}} Q_{k}\left(s^{\prime}, a^{\prime}\right)\right]\]

<h3 id="deep-q-learning">Deep Q-Learning</h3>
<p>In tabular Q-Learning, we need to keep track of a Q-value for each pair $(s, a)$. However, as problems become more realistic, and thus interesting, this becomes intractable. The idea is to have, instead of a table, a <strong>parametrized Q-function $Q_\theta(s, a)$</strong>.</p>

<p>In the case of Deep Q-Learning, this function will be a neural network. The update rule will not be about updating the entry corresponding to the $(s, a)$ pair in a table as it was done in previous euqations, but to update the weights $\theta$ of the neural network,</p>

\[\theta_{k+1} \leftarrow \theta_{k}-\left.\alpha \nabla_{\theta}\left[\frac{1}{2}\left(Q_{\theta}(s, a)-\left[R\left(s, a, s^{\prime}\right)+\gamma \max _{a^{\prime}} Q_{\theta_{k}}\left(s^{\prime}, a^{\prime}\right)\right]\right)^{2}\right]\right|_{\theta=\theta_{k}}\]

<h2 id="deep-q-network-dqn">Deep Q-Network (DQN)</h2>
<p>Now that we have reviewed some theory, let’s start practicing!</p>

<p><strong>Implement a Deep Q-Network</strong> that takes as input the current state $s_t$ and outputs $Q_\theta(s_t, a)$ for each available action $a$. Be careful of the dimensions of the observation tensor that will be fed to your network, and think about the desired output dimension. As a sanity check, verify you can pass it one observation from your gym environment.</p>

<h2 id="dqn-training">DQN Training</h2>

<h3 id="setting-up-training">Setting up training</h3>
<p>You must fill in the following code. Different parts are annotated. Refer to the following subsections for more details.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create the environment
</span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="sh">'</span><span class="s">CartPole-v0</span><span class="sh">'</span><span class="p">).</span><span class="n">unwrapped</span>

<span class="c1"># Set up matplotlib
</span><span class="n">is_ipython</span> <span class="o">=</span> <span class="sh">'</span><span class="s">inline</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">matplotlib</span><span class="p">.</span><span class="nf">get_backend</span><span class="p">()</span>
<span class="k">if</span> <span class="n">is_ipython</span><span class="p">:</span>
    <span class="kn">from</span> <span class="n">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ion</span><span class="p">()</span>

<span class="c1"># If gpu is to be used
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Reset the environment
</span><span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

<span class="c1"># Useful hyperparameters (try to play with them)
</span><span class="n">EPISODES</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># number of episodes
</span><span class="n">EPS_START</span> <span class="o">=</span> <span class="mf">0.9</span>  <span class="c1"># e-greedy threshold start value
</span><span class="n">EPS_END</span> <span class="o">=</span> <span class="mf">0.05</span>  <span class="c1"># e-greedy threshold end value
</span><span class="n">EPS_DECAY</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># e-greedy threshold decay
</span><span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.8</span>  <span class="c1"># Q-learning discount factor
</span><span class="n">LR</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># NN optimizer learning rate
</span><span class="n">HIDDEN_LAYER</span> <span class="o">=</span> <span class="mi">256</span>  <span class="c1"># NN hidden layer size
</span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># Q-learning batch size
</span><span class="n">TARGET_UPDATE</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Update frequency of the target network
</span>
<span class="c1"># Get number of actions from gym action space
</span><span class="n">n_actions</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>

<span class="c1">##############################################
# 1. Instantiate your DQN and the target network
##############################################
</span>
<span class="c1">##############################################
# 2. Create your replay memory
##############################################
</span>
<span class="c1">##############################################
# 3. Choose and instantiate your optimizer
##############################################
</span>
<span class="c1"># Count steps and episode durations
</span><span class="n">steps_done</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">episode_durations</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>
<h4 id="1-target-network">1. Target Network</h4>
<p>Deep Q-Learning can be quite unstable. Thus, a trick is to instantiate one DQN, and another copy of the same model, known as <strong>target network</strong>. The parameters of the latter will be updated every $n$ steps to match the parameters of the DQN you optimize. The target network is used to compute the target Q-value in the update rule.</p>

<h4 id="2-replay-buffer">2. Replay Buffer</h4>
<p>In RL, as training data is collected while interacting with the environment, samples are correlated. The fundamental assumption of i.i.d (idependently and identically distributed) samples is thus violated.</p>

<!-- The i.i.d (idependently and identically distributed) assumption that is a basis of supervised machine learning is thus violated.  -->

<p>In order to get more decorrelated samples, an adopted solution is to rely on a <strong>replay memory</strong> where observations are stored while interacting to be re-used later during training. This has been shown to stabilize training and improve the downstream performance of DQN agents.</p>

<p>You can use the following python class from the <a href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html">PyTorch Tutorial</a></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span><span class="p">,</span> <span class="n">deque</span>
<span class="n">Transition</span> <span class="o">=</span> <span class="nf">namedtuple</span><span class="p">(</span><span class="sh">'</span><span class="s">Transition</span><span class="sh">'</span><span class="p">,</span>
                        <span class="p">(</span><span class="sh">'</span><span class="s">state</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">action</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">next_state</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">reward</span><span class="sh">'</span><span class="p">))</span>

<span class="k">class</span> <span class="nc">ReplayMemory</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">capacity</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">([],</span><span class="n">maxlen</span><span class="o">=</span><span class="n">capacity</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Save a transition</span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">Transition</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="3-optimizer">3. Optimizer</h4>
<p>In the original <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">paper</a> introducing DQN to play Atari Games, authors chose to use the <strong>RMSProp</strong> optimizer. You should start by trying the same as well.</p>

<h3 id="acting-in-the-environment">Acting in the environment</h3>
<p>Our agent must take actions in the environment. As a common practice, we will adopt the $\epsilon$-greedy approach, where we select a random action with probability $\epsilon$, and otherwise the action $a$ with highest Q-value when being in state $s_t$ according to our DQN.</p>

<p>Implement the following function to take actions,</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">steps_done</span>
    <span class="n">steps_done</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="c1">##############################################
</span>    <span class="c1"># Random action with proba epsilon and argmax DQN(s,a) otherwise
</span>    <span class="c1">##############################################
</span>
</code></pre></div></div>

<h3 id="optimizing-our-dqn">Optimizing our DQN</h3>
<p>You must now fill in the function that takes care of optimizing the network.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">optimize_model</span><span class="p">():</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">transitions</span> <span class="o">=</span> <span class="n">memory</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="c1"># Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for
</span>    <span class="c1"># detailed explanation). This converts batch-array of Transitions
</span>    <span class="c1"># to Transition of batch-arrays.
</span>    <span class="n">batch</span> <span class="o">=</span> <span class="nc">Transition</span><span class="p">(</span><span class="o">*</span><span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">transitions</span><span class="p">))</span>

    <span class="c1"># Compute a mask of non-final states and concatenate the batch elements
</span>    <span class="c1"># (a final state would've been the one after which simulation ended)
</span>    <span class="n">non_final_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="nf">tuple</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">,</span>
                                          <span class="n">batch</span><span class="p">.</span><span class="n">next_state</span><span class="p">)),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">)</span>
    <span class="n">non_final_next_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">.</span><span class="n">next_state</span>
                                                <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">])</span>
    <span class="n">state_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">state</span><span class="p">)</span>
    <span class="n">action_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">action</span><span class="p">)</span>
    <span class="n">reward_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">reward</span><span class="p">)</span>

    <span class="c1">##############################################
</span>    <span class="c1"># Compute Q(s_t, a) from your DQN model
</span>    <span class="c1"># state_action_values = ...
</span>    <span class="c1">##############################################
</span>
    <span class="c1">##############################################
</span>    <span class="c1"># Compute the target for your loss (that you will compare to state_action_values)
</span>    <span class="c1"># expected_state_action_values = ...
</span>    <span class="c1">##############################################
</span>    
    <span class="c1">##############################################
</span>    <span class="c1"># Compute Huber loss
</span>    <span class="c1"># criterion = ...
</span>    <span class="c1"># loss = ...
</span>    <span class="c1">##############################################
</span>
    <span class="c1"># Optimize the model
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">policy_net</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
        <span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">clamp_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="huber-loss">Huber Loss</h4>
<p>In the original <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">paper</a> introducing DQN to play Atari Games, authors chose to use the <strong>Huber Loss</strong>. Try to do the same. Note that another name for such loss is <strong>SmoothL1Loss</strong>.</p>

<h3 id="training-loop">Training loop</h3>
<p>You can now use the following training code and start optimizing your agent.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_durations</span><span class="p">():</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">clf</span><span class="p">()</span>
    <span class="n">durations_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">episode_durations</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Training...</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Episode</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Duration</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">durations_t</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    <span class="c1"># Take 100 episode averages and plot them too
</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">durations_t</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">100</span><span class="p">:</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">durations_t</span><span class="p">.</span><span class="nf">unfold</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">means</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">99</span><span class="p">),</span> <span class="n">means</span><span class="p">))</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">means</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">pause</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># pause a bit so that plots are updated
</span>    <span class="k">if</span> <span class="n">is_ipython</span><span class="p">:</span>
        <span class="n">display</span><span class="p">.</span><span class="nf">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">display</span><span class="p">.</span><span class="nf">display</span><span class="p">(</span><span class="n">plt</span><span class="p">.</span><span class="nf">gcf</span><span class="p">())</span>

<span class="k">for</span> <span class="n">i_episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">EPISODES</span><span class="p">):</span>
    <span class="c1"># Initialize the environment and state
</span>    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">count</span><span class="p">():</span>
        <span class="n">env</span><span class="p">.</span><span class="nf">render</span><span class="p">()</span>

        <span class="c1"># Select and perform an action
</span>        <span class="n">action</span> <span class="o">=</span> <span class="nf">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">n_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">reward</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Observe new state
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">n_state</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="c1"># Store the transition in memory
</span>        <span class="n">memory</span><span class="p">.</span><span class="nf">push</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>

        <span class="c1"># Move to the next state
</span>        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

        <span class="c1"># Perform one step of the optimization (on the policy network)
</span>        <span class="nf">optimize_model</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="n">episode_durations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="nf">plot_durations</span><span class="p">()</span>
            <span class="k">break</span>
    <span class="c1"># Update the target network, copying all weights and biases in DQN
</span>    <span class="k">if</span> <span class="n">i_episode</span> <span class="o">%</span> <span class="n">TARGET_UPDATE</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">target_net</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">policy_net</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Complete</span><span class="sh">'</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="nf">render</span><span class="p">()</span>
<span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ioff</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="cartpole-with-screen-images-as-input">Cartpole with screen images as input</h2>
<p>If you have time, you can try to solve the cartpole problem, without relying on the state of the cartpole, but rather the image on the screen. This will mainly involve implementing a Convolutional DQN, and maybe playing with some of the hyperparameters…</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Pierre  Marza.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib">
  </d-bibliography>

</html>
